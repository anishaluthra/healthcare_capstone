{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiabetesAllDummy = pd.read_csv('DiabetesAllDummy.csv', index_col=0)\n",
    "#DiabetesOrdMed = pd.read_csv('DiabetesOrdMed.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a random index of 20% of the values in our dataset. This is the test dataset and will *only* be used for testing our models (sparingly!)\n",
    "np.random.seed(100)\n",
    "length = len(DiabetesAllDummy)\n",
    "testIdx = np.random.choice(range(length), size=int(round(0.2*length)), replace=False)\n",
    "trainIdx = list(set(range(length))-set(testIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just start by looking at a simple linear regression:\n",
    "#Yes, we have a categorical variable as our output. This will not be valid, but will show right away variables which could be important:\n",
    "\n",
    "DiabetesTrain = DiabetesAllDummy[DiabetesAllDummy['IsTrain']==1]\n",
    "DiabetesTrain.index = list(range(len(DiabetesTrain)))\n",
    "\n",
    "DiabetesTest = DiabetesAllDummy[DiabetesAllDummy['IsTrain']==0]\n",
    "DiabetesTest.index = list(range(len(DiabetesTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try simple K-fold (5) random forest classifier using default conditions:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "rfc = rfc()\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrain.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrain['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTest.drop('readmitted', axis=1)\n",
    "testY = DiabetesTest['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    rfc.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    p = rfc.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = rfc.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = rfc.score(testX, testY)\n",
    "    TrainScore = rfc.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = rfc.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9803486974573493,\n",
       " 0.9801926108605054,\n",
       " 0.9794277865359702,\n",
       " 0.9797555683893424,\n",
       " 0.9798027096210277]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the internal train scores, based on the logistic regression\n",
    "TrainScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.883373915215084,\n",
       " 0.8875569707186115,\n",
       " 0.8841231191858651,\n",
       " 0.8844352875070238,\n",
       " 0.885551948051948]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the K-test scores, based on the logistic regression\n",
    "ScoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8849725411882177,\n",
       " 0.8853220169745382,\n",
       " 0.8845232151772342,\n",
       " 0.8845731402895657,\n",
       " 0.8846729905142287]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the scores for each of the 5 ensemble models, compared to the untouched 20% test index\n",
    "PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the prediction lists from the train, compared to actual Y values for the train:\n",
    "PredictList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8865441286649018"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These values may sound impressive, but they absolutely aren't. 11.1 percent of patients return under 30 days, so simply guessing \"No\" will get you 88.8% accuracy:\n",
    "1-np.sum(trainY)/len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.886540652754148,\n",
       " 0.8857133937908752,\n",
       " 0.8867123480106763,\n",
       " 0.8863533488379353,\n",
       " 0.8863239058500343]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8856215271274271,\n",
       " 0.8886183430105513,\n",
       " 0.8848098894924143,\n",
       " 0.8861834301055129,\n",
       " 0.8861138861138861]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ScoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8865202196704942,\n",
       " 0.8861208187718422,\n",
       " 0.8861707438841737,\n",
       " 0.8863205192211683,\n",
       " 0.8864203694458312]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy with reduced features was no better than the overall accuracy with all features. Let's check what might be going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(PredictList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9086"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16017"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PredictList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This seems to be the biggest problem. Out of ~16,000 observations (in the first K group, as an example), only 78 of them\\nwere predicted to return within 30 days. We know that 11% should, or rougly 1,800. We need to tune this model to be more\\ngenerous in predicting a positive outcome'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This seems to be the biggest problem. Out of ~16,000 observations (in the first K group, as an example), only 78 of them\n",
    "were predicted to return within 30 days. We know that 11% should, or rougly 1,800. We need to tune this model to be more\n",
    "generous in predicting a positive outcome'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  35 out of  35 | elapsed:   27.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.001}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#We will perform a grid search to find the optimal hyperparameters for our RF algorithm and test using our K-fold data:\n",
    "\n",
    "# C value is the one most important for tuning a logistic regression. Let's see how varying this value affects the score:\n",
    "C_range = [0.001, 0.01, 0.1, 1, 10, 100, 100]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'C': C_range}\n",
    "\n",
    "# Use the random grid to search for best C hyperparameter:\n",
    "\n",
    "# First create the base model to tune\n",
    "lgrCV = lgr()\n",
    "\n",
    "# Random search of parameters, using 5-fold cross validation, \n",
    "lgr_random = RandomizedSearchCV(estimator = lgrCV, param_distributions = random_grid, n_iter = 7, cv = 5, verbose=2, random_state=42, n_jobs = 2)\n",
    "\n",
    "# Fit the random search model\n",
    "lgr_random.fit(trainX, trainY)\n",
    "\n",
    "#Then print the best parameters using best_params_\n",
    "lgr_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try the logistic regression with c=0.001 and see how that works:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as lgr\n",
    "\n",
    "lgr = lgr()\n",
    "lgr.set_params(C=0.001)\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrain.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrain['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTest.drop('readmitted', axis=1)\n",
    "testY = DiabetesTest['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    lgr.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    p = lgr.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = lgr.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = lgr.score(testX, testY)\n",
    "    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = lgr.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8860708936595107,\n",
       " 0.8857713429855217,\n",
       " 0.8858711932101847,\n",
       " 0.8858212680978532,\n",
       " 0.8858212680978532]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Either logistic regression does not work for this data, or something is going wrong here. The predicted scores are up by the slightest amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, try the logistic regression with c=0.001 and reduced features and see how that works:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as lgr\n",
    "\n",
    "lgr = lgr()\n",
    "lgr.set_params(C=0.001)\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrainM.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrainM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTestM.drop('readmitted', axis=1)\n",
    "testY = DiabetesTestM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    lgr.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    p = lgr.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = lgr.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = lgr.score(testX, testY)\n",
    "    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = lgr.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8863704443334998,\n",
       " 0.8865202196704942,\n",
       " 0.8865202196704942,\n",
       " 0.8865701447828258,\n",
       " 0.8865701447828258]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, try the logistic regression with c=0.001 and reduced features and see how that works:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as lgr\n",
    "\n",
    "lgr = lgr()\n",
    "lgr.set_params(C=1000)\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrainM.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrainM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTestM.drop('readmitted', axis=1)\n",
    "testY = DiabetesTestM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    lgr.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    p = lgr.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = lgr.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = lgr.score(testX, testY)\n",
    "    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = lgr.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8865202196704942,\n",
       " 0.8862206689965052,\n",
       " 0.8861707438841737,\n",
       " 0.8863205192211683,\n",
       " 0.8863205192211683]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(PredictList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
